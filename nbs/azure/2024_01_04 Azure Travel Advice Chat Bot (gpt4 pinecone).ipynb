{"cells":[{"cell_type":"markdown","id":"59c1b467","metadata":{"id":"59c1b467"},"source":["# Pinecone DatabaseManager\n","\n","> This will be the interface between an application and the databse.  \n","\n","> In the immediate, it will initialise the database, read the Travel Advice JSON, ingest it via chunks into the database and it will perform searches given an embedding."]},{"cell_type":"code","execution_count":null,"id":"b81c3e9c","metadata":{"id":"b81c3e9c"},"outputs":[],"source":["#| default_exp DatabaseManager"]},{"cell_type":"code","source":[],"metadata":{"id":"NbuwnFG-E-nc"},"id":"NbuwnFG-E-nc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"0ThpqLT5edQe"},"id":"0ThpqLT5edQe"},{"cell_type":"code","execution_count":null,"id":"17d03570","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17d03570","executionInfo":{"status":"ok","timestamp":1701703844851,"user_tz":0,"elapsed":66616,"user":{"displayName":"David Farrell","userId":"17580088866539757700"}},"outputId":"1df5fded-c129-4c27-df7f-35a4fac06148"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting cohere\n","  Downloading cohere-4.37-py3-none-any.whl (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m847.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken\n","  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.1)\n","Collecting backoff<3.0,>=2.0 (from cohere)\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Collecting fastavro<2.0,>=1.8 (from cohere)\n","  Downloading fastavro-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n","Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.11.17)\n","Installing collected packages: fastavro, backoff, tiktoken, cohere\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed backoff-2.2.1 cohere-4.37 fastavro-1.9.0 tiktoken-0.5.2\n","Collecting openai==0.28\n","  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m961.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n","Installing collected packages: openai\n","Successfully installed openai-0.28.0\n","Collecting fastapi\n","  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-multipart\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn\n","  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.7.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.13)\n","Collecting starlette<0.28.0,>=0.27.0 (from fastapi)\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions>=4.8.0 (from fastapi)\n","  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.6)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n","Installing collected packages: typing-extensions, python-multipart, h11, uvicorn, starlette, fastapi\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed fastapi-0.104.1 h11-0.14.0 python-multipart-0.0.6 starlette-0.27.0 typing-extensions-4.8.0 uvicorn-0.24.0.post1\n","Collecting kaleido\n","  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: kaleido\n","Successfully installed kaleido-0.2.1\n","Collecting pinecone-client==2.2.1\n","  Downloading pinecone_client-2.2.1-py3-none-any.whl (177 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.1) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.1) (6.0.1)\n","Collecting loguru>=0.5.0 (from pinecone-client==2.2.1)\n","  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.1) (4.8.0)\n","Collecting dnspython>=2.0.0 (from pinecone-client==2.2.1)\n","  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.1) (2.8.2)\n","Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.1) (2.0.7)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.1) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.1) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client==2.2.1) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.1) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.1) (2023.11.17)\n","Installing collected packages: loguru, dnspython, pinecone-client\n","Successfully installed dnspython-2.4.2 loguru-0.7.2 pinecone-client-2.2.1\n","Collecting datasets\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting pyarrow-hotfix (from datasets)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.8.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n","Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n","Collecting unidecode\n","  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.7\n","Collecting gradio\n","  Downloading gradio-4.7.1-py3-none-any.whl (16.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n","Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.104.1)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.7.0 (from gradio)\n","  Downloading gradio_client-0.7.0-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx (from gradio)\n","  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.19.4)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n","Collecting pydantic>=2.0 (from gradio)\n","  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n","Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.8.0)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.0.post1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (2023.6.0)\n","Collecting websockets<12.0,>=10.0 (from gradio-client==0.7.0->gradio)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.13.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.45.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n","Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio)\n","  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n","Collecting pydantic-core==2.14.5 (from pydantic>=2.0->gradio)\n","  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.11.17)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n","Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n","Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n","Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n","Collecting httpcore==1.* (from httpx->gradio)\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.2.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.31.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n","Building wheels for collected packages: ffmpy\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=6e7a2cbd83810283bef7649e1f18755b1cf3180467f29469325a7e13586f3401\n","  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n","Successfully built ffmpy\n","Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, pydantic-core, orjson, httpcore, colorama, annotated-types, aiofiles, pydantic, httpx, gradio-client, gradio\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 1.10.13\n","    Uninstalling pydantic-1.10.13:\n","      Successfully uninstalled pydantic-1.10.13\n","Successfully installed aiofiles-23.2.1 annotated-types-0.6.0 colorama-0.4.6 ffmpy-0.3.1 gradio-4.7.1 gradio-client-0.7.0 httpcore-1.0.2 httpx-0.25.2 orjson-3.9.10 pydantic-2.5.2 pydantic-core-2.14.5 pydub-0.25.1 semantic-version-2.10.0 shellingham-1.5.4 tomlkit-0.12.0 websockets-11.0.3\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (4.8.0)\n"]}],"source":["%pip install cohere tiktoken\n","%pip install openai==0.28\n","%pip install fastapi python-multipart uvicorn\n","%pip install kaleido\n","\n","%pip install pinecone-client==2.2.1\n","%pip install datasets\n","%pip install unidecode\n","%pip install --upgrade gradio\n","%pip install --upgrade typing_extensions\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"JQrCPHn-d-Os"},"id":"JQrCPHn-d-Os","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"857d23f1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"857d23f1","executionInfo":{"status":"ok","timestamp":1701703848779,"user_tz":0,"elapsed":3974,"user":{"displayName":"David Farrell","userId":"17580088866539757700"}},"outputId":"e004330c-cb4e-4e99-fa1e-7ca136578665"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n\\n\\n%pip install -qU   tiktoken==0.4.0   openai==0.27.7   langchain==0.0.179   pinecone-client==2.2.1   datasets==2.13.1   cohere\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["#| export\n","import json\n","from bs4 import BeautifulSoup\n","import re\n","#import pymilvus\n","#from pymilvus import Collection, CollectionSchema, FieldSchema, DataType, connections, utility\n","\n","import pinecone\n","from datasets import load_dataset, Dataset\n","import unidecode\n","\n","#for embeddings\n","import os\n","import openai\n","import time # to step back in case of rate limit\n","from tqdm.auto import tqdm # for progress bars\n","import tiktoken\n","\n","from IPython.display import Markdown\n","import gradio as gr\n","\n","import random\n","'''\n","\n","\n","%pip install -qU \\\n","  tiktoken==0.4.0 \\\n","  openai==0.27.7 \\\n","  langchain==0.0.179 \\\n","  pinecone-client==2.2.1 \\\n","  datasets==2.13.1 \\\n","  cohere\n","'''"]},{"cell_type":"code","execution_count":null,"id":"2f9c947e","metadata":{"id":"2f9c947e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d0d537b9","metadata":{"id":"d0d537b9"},"outputs":[],"source":["#| export\n","class DatabaseManager:\n","    def __init__(self, config):\n","        \"\"\"\n","        Initialize the DatabaseManager with configuration settings.\n","        :param config: Configuration details for database connection and other settings.\n","        \"\"\"\n","        self.config = config\n","        # Initialize database connection here\n","\n","\n","\n","    def chunk_json(self, json_data):\n","        \"\"\"\n","        Divide the JSON data into manageable chunks.\n","        :param json_data: The parsed JSON data.\n","        :return: List of chunks.\n","        \"\"\"\n","        # Implement chunking logic here\n","        chunks = []\n","        return chunks\n","\n","    def embed_chunks(self, chunks):\n","        \"\"\"\n","        Create embeddings for each chunk of data.\n","        :param chunks: List of data chunks.\n","        :return: List of embedded chunks.\n","        \"\"\"\n","        # Implement embedding logic here\n","        embedded_chunks = []\n","        return embedded_chunks\n","\n","    def initialize_database(self):\n","        \"\"\"\n","        Set up the Milvus database, including connection and schema.\n","        \"\"\"\n","        # Implement database initialization here\n","\n","    def store_in_milvus(self, embedded_chunks):\n","        \"\"\"\n","        Store embedded chunks in the Milvus database.\n","        :param embedded_chunks: List of embedded chunks.\n","        \"\"\"\n","        # Implement storage logic here\n","\n","    def search_database(self, query, k):\n","        \"\"\"\n","        Search the database for K nearest chunks based on the query embedding.\n","        :param query: Search query.\n","        :param k: Number of nearest chunks to find.\n","        :return: Search results.\n","        \"\"\"\n","        # Implement search logic here\n","\n","    def retrieve_data(self, search_results):\n","        \"\"\"\n","        Fetch chunk data and metadata based on search results.\n","        :param search_results: Results from the database search.\n","        :return: Corresponding data and metadata.\n","        \"\"\"\n","        # Implement data retrieval logic here\n","\n","    def __del__(self):\n","        \"\"\"\n","        Cleanup when an instance is destroyed, like closing database connections.\n","        \"\"\"\n","        # Implement cleanup logic here\n","\n","    def ingest_json(self, file_path):\n","        \"\"\"\n","        Read and parse a JSON file.\n","        :param file_path: Path to the JSON file.\n","        :return: Parsed JSON data.\n","        \"\"\"\n","        with open(file_path, 'r') as file:\n","            data = json.load(file)\n","        return data\n","\n"]},{"cell_type":"markdown","id":"c5294349","metadata":{"id":"c5294349"},"source":["## Configure the database and load the data\n","\n","I'm not sure how we'll do it in the future.\n","\n","For time being, we are going to do two things now\n","1. ingest the travel advice json\n","2. turn that into a local database\n","   \n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"eed30828","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eed30828","executionInfo":{"status":"ok","timestamp":1701703849448,"user_tz":0,"elapsed":688,"user":{"displayName":"David Farrell","userId":"17580088866539757700"}},"outputId":"f4bf3f84-d65f-47b8-b8d4-311467fb07fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"object\": \"list\",\n","  \"data\": [\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-search-babbage-doc-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"curie-search-query\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-davinci-003\",\n","      \"ready\": true,\n","      \"owner\": \"openai-internal\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-search-babbage-query-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"babbage\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"babbage-search-query\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-babbage-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-similarity-davinci-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"davinci-similarity\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"code-davinci-edit-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"curie-similarity\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"babbage-search-document\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"curie-instruct-beta\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-search-ada-doc-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"davinci-instruct-beta\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-3.5-turbo-instruct\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-similarity-babbage-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-search-davinci-doc-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"babbage-similarity\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-embedding-ada-002\",\n","      \"ready\": true,\n","      \"owner\": \"openai-internal\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"davinci-search-query\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-similarity-curie-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-davinci-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-3.5-turbo-instruct-0914\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-search-davinci-query-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"ada-search-document\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"ada-code-search-code\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"babbage-002\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-4-1106-preview\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"davinci-002\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-3.5-turbo-0613\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"davinci-search-document\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"curie-search-document\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"babbage-code-search-code\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-search-ada-query-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"code-search-ada-text-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"babbage-code-search-text\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-3.5-turbo-0301\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"code-search-babbage-code-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"ada-search-query\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-3.5-turbo\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"ada-code-search-text\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"tts-1-hd\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-3.5-turbo-16k-0613\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-search-curie-query-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-3.5-turbo-1106\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-davinci-002\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-davinci-edit-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"code-search-babbage-text-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"ada\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-ada-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"ada-similarity\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"code-search-ada-code-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-similarity-ada-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-3.5-turbo-16k\",\n","      \"ready\": true,\n","      \"owner\": \"openai-internal\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-search-curie-doc-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai-dev\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-4-vision-preview\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"text-curie-001\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"curie\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-4\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"tts-1\",\n","      \"ready\": true,\n","      \"owner\": \"openai-internal\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"whisper-1\",\n","      \"ready\": true,\n","      \"owner\": \"openai-internal\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-4-0314\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"davinci\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"dall-e-2\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"tts-1-1106\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"tts-1-hd-1106\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"dall-e-3\",\n","      \"ready\": true,\n","      \"owner\": \"system\",\n","      \"permissions\": null,\n","      \"created\": null\n","    },\n","    {\n","      \"object\": \"engine\",\n","      \"id\": \"gpt-4-0613\",\n","      \"ready\": true,\n","      \"owner\": \"openai\",\n","      \"permissions\": null,\n","      \"created\": null\n","    }\n","  ]\n","}\n"]}],"source":["#get ready for embeddings\n","\n","# this stuff here will be useful if we decide to chunk parts into sub-parts\n","#tokenizer_name = tiktoken.encoding_for_model('gpt-4')\n","#tokenizer_name.name\n","#tokenizer = tiktoken.get_encoding(tokenizer_name.name)\n","\n","\n","# get API key from top-right dropdown on OpenAI website\n","openai.api_key =  \"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\"\n","print (openai.Engine.list())  # check we have authenticated\n","\n","embed_model = \"text-embedding-ada-002\"\n","\n","res = openai.Embedding.create(\n","    input=[\n","        \"Sample document text goes here\",\n","        \"there will be several phrases in each batch\"\n","    ], engine=embed_model\n",")\n","res.keys()\n","\n","# we need to tell Milvus later, the size of our embdding model\n","VECTOR_DIM = len(res['data'][0]['embedding'])\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"e0319b48","metadata":{"id":"e0319b48"},"outputs":[],"source":["config = {\n","    'travel_file':  \"./ingest_data_sources/travel-advice-all-countries.json\"\n","}\n","\n","db_manager = DatabaseManager(config)\n"]},{"cell_type":"markdown","id":"4bc18ae9","metadata":{"id":"4bc18ae9"},"source":["### Ingesting\n","\n","Here's what I'm doing\n","\n","1. We load the travel advice json into memory (data)\n","2. We loop around it to turn it into a new structure that better fits our needs.\n","\n","   Each item will have this structure:\n","   ```json\n","   {\n","       \"url\": \"/foreign-travel-advice/british-indian-ocean-territory\",\n","       \"country_name\": \"British Indian Ocean Territory\",\n","       \"content_title\": \"Summary\",\n","       \"part_id\": 0,\n","       \"content\": \"Before you travel, check the 'Entry requirements'....\",\n","       \"content_html\": \"\\n<div class=\\\"example\\\">\\n<p>Before you travel, ...\",\n","       \"content_embedding\": 0   <- some embedding\n","   }\n","\n","3. Note that for any individual country there are several \"parts\" - this is why we have so much metadata. It's so we can pull back each chunk.\n","4. Note also that we do a bunch of embedding here.\n","4.  We save the new formatted json as chunk_json_format.json in case we want to skip all this in the future - for time being, I'm assuming a 'dump' of this may change.\n"]},{"cell_type":"code","execution_count":null,"id":"1882a7d0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1882a7d0","executionInfo":{"status":"ok","timestamp":1701703850735,"user_tz":0,"elapsed":1297,"user":{"displayName":"David Farrell","userId":"17580088866539757700"}},"outputId":"c99d71bc-b9d7-4d80-e2a6-e5a9aed33272"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'dimension': 1536,\n"," 'index_fullness': 0.1,\n"," 'namespaces': {'': {'vector_count': 2458}},\n"," 'total_vector_count': 2458}"]},"metadata":{},"execution_count":7}],"source":["# Let's set up pinecone\n","\n","# initialize connection to pinecone (get API key at app.pinecone.io)\n","# cyb api_key = \"31c9a1df-44c8-4bea-b4dd-675aee80808f\" #os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n","api_key = \"9e15686d-a95d-47f8-93d1-4920e1259e74\"\n","\n","# find your environment next to the api key in pinecone console\n","env = \"us-west4-gcp-free\" #os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\n","\n","pinecone.init(api_key=api_key, environment=env)\n","pinecone.whoami()\n","\n","index_name = \"fcdo-travel\"\n","\n","\n","# check if index already exists (it shouldn't if this is first time)\n","if index_name not in pinecone.list_indexes():\n","    # if does not exist, create index\n","    pinecone.create_index(\n","        index_name,\n","        dimension=len(res['data'][0]['embedding']),\n","        metric='cosine'\n","    )\n","    # wait for index to be initialized\n","    while not pinecone.describe_index(index_name).status['ready']:\n","        time.sleep(1)\n","\n","# connect to index\n","pinecone_index = pinecone.Index(index_name)\n","# view index stats\n","pinecone_index.describe_index_stats()"]},{"cell_type":"code","execution_count":null,"id":"44ab87a3","metadata":{"id":"44ab87a3"},"outputs":[],"source":["\n","# THIS TAKES FOREVER TO RUN BECAUSE IT EMBEDS EVERY ONE OF THE CHUNKS!\n","# only run if you need to make a database\n","\n","hydrate_database = False\n","\n","\n","# Define a function to split the content into chunks of max 500 words\n","# I had hoped that chunking by part was enough but I went over the pinecone non-vector size limit.\n","# I can't use the chunker I used before because it was langchain - I asume we can get a better, token based one from griptape\n","def split_content(content, max_words=500):\n","    words = content.split()\n","    for i in range(0, len(words), max_words):\n","        yield ' '.join(words[i:i + max_words])\n","\n","# Function to sanitize unique_id to be ASCII -  pinecone id's have to be in ascii\n","def sanitize_id(unsanitized_id):\n","    return unidecode.unidecode(unsanitized_id)\n","\n","if hydrate_database:\n","    data = db_manager.ingest_json(db_manager.config['travel_file'])\n","    new_data_structure = []\n","    batch_size = 100\n","\n","    # Initialize a dictionary to keep track of the maximum lengths\n","    # This is inefficient, but I'm not sure as yet how big this data is (particularly each 'content') so I'm just getting to know the data\n","    # We'll possibly replace this with a sensible chunking size and split one part over mutliples - possibly with ref to each other.\n","    max_lengths = {\n","        \"url\": 0,\n","        \"country_name\": 0,\n","        \"content_title\": 0,\n","        \"part_id\": 0,  # Assuming you want to track the maximum integer value\n","        \"content_to_embed\": 0,\n","        \"content_html\": 0,\n","        \"content_embedding\": 0\n","    }\n","\n","\n","    # Loop around each element in data\n","    for element in data:\n","        base_path = element[\"countryInfo\"][\"base_path\"]\n","        country_name = element[\"countryInfo\"][\"details\"][\"country\"][\"name\"]\n","\n","        # Loop around each part in each element\n","        for index, part in enumerate(element[\"countryInfo\"][\"details\"][\"parts\"]):\n","            html_content = part[\"body\"]\n","            soup = BeautifulSoup(html_content, 'html.parser')\n","            text_content = soup.get_text()\n","\n","            # Replace newlines and tabs with a space, and strip leading/trailing whitespaces\n","            text_content = re.sub(r'\\s+', ' ', text_content).strip()\n","\n","            # Replace the Unicode characters I found with ASCII equivalents (maybe there are others)\n","            replacements = {\n","                '\\u2018': \"'\", '\\u2019': \"'\",  # Single quotes\n","                '\\u201c': '\"', '\\u201d': '\"',  # Double quotes\n","                '\\u2026': '...',               # Ellipsis (add anything else as required)\n","            }\n","            for unicode_char, ascii_char in replacements.items():\n","                text_content = text_content.replace(unicode_char, ascii_char)\n","\n","            # Generate a unique ID for each element\n","            unique_id = f\"{country_name}_{index}\"\n","            unique_id = sanitize_id(unique_id)\n","            content_to_embed = f\"Country:'{country_name}'; Section:'{part['title']}'; Content:{text_content}\"\n","\n","            if len(content_to_embed.split()) > 500:\n","                # Split the part into sub-parts\n","                for sub_index, sub_content in enumerate(split_content(content_to_embed)):\n","                    # Generate a new unique ID for each sub-part\n","                    unique_id = f\"{country_name}_{index}_{sub_index}\"\n","                    unique_id = sanitize_id(unique_id)\n","                    # this is the real data - but we embed based on embeds\n","                    new_element = {\n","                        \"url\": base_path,\n","                        \"country_name\": country_name,\n","                        \"content_title\": part[\"title\"],\n","                        \"part_id\": index,\n","                        \"content_to_embed\": sub_content\n","                    }\n","\n","                    print (f\"embed (SUB) {sub_content}\")\n","                    try:\n","                        res = openai.Embedding.create(input=new_element['content_to_embed'], engine=embed_model)\n","                        successful_embed = True\n","                    except:\n","                        done = False\n","                        while not done:\n","                            time.sleep(5)\n","                            try:\n","                                res = openai.Embedding.create(input=new_element['content_to_embed'], engine=embed_model)\n","                                done = True\n","                            except:\n","                                pass\n","                    embed = res['data'][0]['embedding']  # Get the first (and only) embedding\n","                    # we can save it next time - but here we have the data to send into pinecone\n","                    new_data_structure.append(new_element)\n","\n","\n","                    # Serialize the metadata to a JSON string\n","                    metadata_json = json.dumps(new_element)\n","\n","                    # Check the size of the metadata\n","                    metadata_size = len(metadata_json.encode('utf-8'))\n","                    print(f\"Metadata size:[{unique_id}][{metadata_size}] bytes\")\n","\n","                    # Only upsert if the metadata size is within the limit\n","                    if metadata_size <= 40960:\n","                        to_upsert = [(unique_id, embed, new_element)]\n","                        pinecone_index.upsert(vectors=to_upsert)\n","                    else:\n","                        print(f\"Metadata for {unique_id} exceeds the Pinecone limit.\")\n","\n","\n","            else:\n","                # ... (your existing embedding and upsert logic here, using the original unique_id and content_to_embed)\n","                # this is the real data - but we embed based on embeds\n","                new_element = {\n","                    \"url\": base_path,\n","                    \"country_name\": country_name,\n","                    \"content_title\": part[\"title\"],\n","                    \"part_id\": index,\n","                    \"content_to_embed\": content_to_embed\n","                }\n","\n","                print (f\"embed {new_element['content_to_embed']}\")\n","                try:\n","                    res = openai.Embedding.create(input=new_element['content_to_embed'], engine=embed_model)\n","                    successful_embed = True\n","                except:\n","                    done = False\n","                    while not done:\n","                        time.sleep(5)\n","                        try:\n","                            res = openai.Embedding.create(input=new_element['content_to_embed'], engine=embed_model)\n","                            done = True\n","                        except:\n","                            pass\n","                embed = res['data'][0]['embedding']  # Get the first (and only) embedding\n","\n","                # we can save it next time - but here we have the data to send into pinecone\n","                new_data_structure.append(new_element)\n","\n","                # Serialize the metadata to a JSON string\n","                metadata_json = json.dumps(new_element)\n","\n","                # Check the size of the metadata\n","                metadata_size = len(metadata_json.encode('utf-8'))\n","                print(f\"Metadata size:[{unique_id}][{metadata_size}] bytes\")\n","\n","                # Only upsert if the metadata size is within the limit\n","                if metadata_size <= 40960:\n","                    to_upsert = [(unique_id, embed, new_element)]\n","                    pinecone_index.upsert(vectors=to_upsert)\n","                else:\n","                    print(f\"Metadata for {unique_id} exceeds the Pinecone limit.\")\n","\n","\n","\n","\n","\n","\n","\n","    new_json_string = json.dumps(new_data_structure, indent=4)\n","\n","    file_name = \"new_chunk_json_format.json\"\n","    with open(file_name, 'w', encoding='utf-8') as file:\n","        file.write(new_json_string)\n","\n","    print (\"Example of new data structure:\\n\")\n","    print(new_json_string[:7000])\n","\n","    #print(new_json_string[:5000])  # Adjust the slice as needed"]},{"cell_type":"code","execution_count":null,"id":"d85d4538","metadata":{"id":"d85d4538"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"382a1423","metadata":{"id":"382a1423"},"outputs":[],"source":["query = \"What do i need to know about the Bir Tawil Trapezoid?\"\n","\n","def retrieve_context(query):\n","    res = openai.Embedding.create(\n","        input=[query],\n","        engine=embed_model\n","    )\n","\n","    # retrieve from Pinecone\n","    xq = res['data'][0]['embedding']\n","\n","    # get relevant contexts (including the questions)\n","    res = pinecone_index.query(xq, top_k=3, include_metadata=True)\n","\n","    # get list of retrieved text\n","    contexts = [item['metadata']['content_to_embed'] for item in res['matches']]\n","    #print res['matches']\n","    return contexts\n","\n","def augment_query(message, history, contexts):\n","    augmented_query = \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+message + \\\n","      \" (remember - use the information above in your answer and reply 'I don't know' if the answer is not contained in that information.  Do not speculate - answer using the information provided or decline. )\"\n","    return augmented_query\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a5ae3f5c","metadata":{"id":"a5ae3f5c"},"outputs":[],"source":["\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f9c4e68a","metadata":{"id":"f9c4e68a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"94bc7099","metadata":{"id":"94bc7099"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"6f4c1d7f","metadata":{"id":"6f4c1d7f"},"outputs":[],"source":["\n","# system message to 'prime' the model\n","primer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n","user questions based on the information provided by the user above\n","each question. If the information can not be found in the information\n","provided by the user you truthfully say \"I don't know\".  It is of the\n","UTMOST IMPORTANCE that you answer \"I don't know\" if the answer to the\n","user's question(s) cannot be found in the information above their questions.\n","If you make a mistake I may be fired so please err on the side of caution.\n","Answer the questions using the information.  Do not 'free wheel' a response.\n","When you answer users questions you provide DIRECT QUOTES from the information\n","provided in your answer.\n","\n","For example if the user provides this information:\n","\n","'\n","FCDO travel advice aims to inform British nationals so they can make decisions about travelling abroad. This page sets out how the FCDO assesses risks and compiles travel advice.\n","'\n","\n","and if the user asks the following question\n","'\n","Why does the FCDO even exist i mean wots the point?\n","'\n","\n","You would answer using the direct quote \"FCDO travel advice aims to inform British nationals so they can make decisions about travelling abroad.\"\n","\n","Below is the user's information and question.\n","\n","\"\"\"\n","\n","previous_answer = \"\"\n","\n","def ask_question(message, history):\n","    contexts = retrieve_context(message)\n","    augmented_query = augment_query(message, history, contexts)\n","    answer = ask_openai(augmented_query, history)\n","    previous_answer = answer;\n","    return answer\n","\n","def get_sources(message, history):\n","    contexts = retrieve_context(message)\n","    return_string = \"\"\n","    for item in contexts:\n","        return_string = f\"{return_string} \\n \\nSource:  {item} --\\n\"\n","\n","    return return_string\n","\n","def ask_openai(message, history):\n","    print (primer)\n","    print (message)\n","    res = openai.ChatCompletion.create(\n","        #model=\"gpt-4\",\n","        model=\"gpt-3.5-turbo-16k-0613\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": primer},\n","            {\"role\": \"user\", \"content\": message}\n","        ]\n","    )\n","    #display(Markdown(res['choices'][0]['message']['content']))\n","    return res['choices'][0]['message']['content']"]},{"cell_type":"code","source":[],"metadata":{"id":"0OZnMwA5w4uO"},"id":"0OZnMwA5w4uO","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UIzKFQM2xUU7"},"id":"UIzKFQM2xUU7","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"225d2dfd","metadata":{"id":"225d2dfd"},"source":["## This one shows SOURCES - the next one is answer bot"]},{"cell_type":"code","execution_count":null,"id":"2371b14a","metadata":{"id":"2371b14a"},"outputs":[],"source":["#gr.ChatInterface(get_sources).launch()"]},{"cell_type":"markdown","source":["## This one below is the Q&A Bot - above is the sources one"],"metadata":{"id":"_h50v-mN0Mpd"},"id":"_h50v-mN0Mpd"},{"cell_type":"code","source":["sample_question = \"We land at on 1st January  and have a return flight from Philippines on 1st Feb. Since we are only overstaying our 30 day visa by 12 hours do we need to pay? And we are flying to Vietnam for 3 days half way through our trip, does our 30 days reset?\"\n"],"metadata":{"id":"b94xt201kahV"},"id":"b94xt201kahV","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"0b5fb256","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":645},"id":"0b5fb256","executionInfo":{"status":"ok","timestamp":1701703854212,"user_tz":0,"elapsed":3264,"user":{"displayName":"David Farrell","userId":"17580088866539757700"}},"outputId":"2c3af611-d199-42e4-859b-916eda29e9e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://d9f446f80dd089fdc0.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://d9f446f80dd089fdc0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":12}],"source":["\n","gr.ChatInterface(ask_question).launch()"]},{"cell_type":"code","execution_count":null,"id":"015b229c","metadata":{"id":"015b229c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3d58df8a","metadata":{"id":"3d58df8a"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"7e31febc","metadata":{"id":"7e31febc"},"source":["#"]},{"cell_type":"code","execution_count":null,"id":"a5ee1d0c","metadata":{"id":"a5ee1d0c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"5805bccb","metadata":{"id":"5805bccb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"c8371e95","metadata":{"id":"c8371e95"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"248caa6b","metadata":{"id":"248caa6b"},"outputs":[],"source":["#| hide\n","# Leave this to the bottom so we auto-export code\n","#import nbdev; nbdev.nbdev_export()"]},{"cell_type":"code","execution_count":null,"id":"d1cc5f26","metadata":{"id":"d1cc5f26"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"fd9db2ed","metadata":{"id":"fd9db2ed"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"python3","language":"python","name":"python3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}